# AML-to-fool-a-CNN

We explore two algorithms (fast gradient signed method or FGSM and projected gradient
descent or PGD) that generate adversarial examples to fool a convolutional neural network (CNN). Then, we apply
adversarial training as a defense mechanism against these two algorithm-generated adversarial examples. We use
the MNIST dataset to explore, train, and test our work.
